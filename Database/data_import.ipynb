{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!pip install psycopg2-binary\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "#!pip install paramiko\n",
    "#!pip install scp\n",
    "from paramiko import SSHClient,AutoAddPolicy\n",
    "from scp import SCPClient\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset data\n",
    "# Better to load in the order proposed by the lines and the comments\n",
    "# Remember to merge common info between the datasets after importing an entire family of datasets\n",
    "\n",
    "#dataset_name = 'UJI1'\n",
    "#dataset_name = 'DSI1' # DSI1 DSI2 'DSI_trajectories'\n",
    "#dataset_name = 'LIB1' # LIB1 LIB2\n",
    "#dataset_name = 'SIM001' # SIM001 # SIM002 # SIM003 # SIM004 # SIM005 # SIM006 # SIM007 # SIM008 # SIM009 # SIM010 \n",
    "#dataset_name = 'UTS'\n",
    "#dataset_name = 'TUT3' # TUT3 TUT4 TUT1 TUT6 (then merge info between datasets)\n",
    "#dataset_name = 'TUT2' # TUT2 TUT5 TUT7 (then merge info between datasets)\n",
    "#dataset_name = 'IPIN21_Track3'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mandatory files\n",
    "\n",
    "dataset_metadata = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/dataset_metadata.csv', sep=',', header=None, na_values = 'NULL', keep_default_na=False)\n",
    "dataset_url = dataset_metadata.iloc[0,0]\n",
    "dataset_notes = dataset_metadata.iloc[0,1]\n",
    "\n",
    "\n",
    "fingerprints_exists = False\n",
    "places_exists = False\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/fingerprints.csv'):\n",
    "    fingerprints_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/fingerprints.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    fingerprints_exists = True\n",
    "    \n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/places.csv'):\n",
    "    places_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/places.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    places_exists = True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optional files\n",
    "devices_exists = False\n",
    "users_exists = False\n",
    "tessellations_exists = False\n",
    "wifiobs_exists = False\n",
    "bluetoothobs_exists = False\n",
    "gnssobs_exists = False\n",
    "imuobs_exists = False\n",
    "adjacences_exists = False\n",
    "\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/devices.csv'):\n",
    "    devices_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/devices.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    devices_exists = True\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/users.csv'):\n",
    "    users_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/users.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    users_exists = True\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/tessellations.csv'):\n",
    "    tessellations_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/tessellations.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    tessellations_exists = True\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/wifi_obs.csv'):   \n",
    "    wifi_obs_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/wifi_obs.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    wifiobs_exists = True\n",
    "\n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/blue_obs.csv'):   \n",
    "    bluetooth_obs_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/blue_obs.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    bluetoothobs_exists = True\n",
    "    \n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/gnss_obs.csv'):   \n",
    "    gnss_obs_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/gnss_obs.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    gnssobs_exists = True\n",
    "    \n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/imu_obs.csv'):   \n",
    "    imu_obs_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/imu_obs.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    imuobs_exists = True\n",
    "    \n",
    "if path.exists('../Datasets/converted_datasets/' + dataset_name + '/adjacences.csv'):   \n",
    "    adjacences_frame = pd.read_csv('../Datasets/converted_datasets/' + dataset_name + '/adjacences.csv', na_values = 'NULL', sep=',', dtype=object)\n",
    "    adjacences_exists = True\n",
    "    \n",
    "    \n",
    "\n",
    "assert fingerprints_exists and places_exists, \"ERROR: missing fingeprints or places mandatory file\"\n",
    "assert not(adjacences_exists and not(places_exists)), \"ERROR: missing places file but adjacences file specified\"\n",
    "\n",
    "\n",
    "\n",
    "# To be used to format possible null values in INSERT queries\n",
    "def format_nan_string(value):\n",
    "    if pd.isna(value):\n",
    "        return 'null'\n",
    "    else:\n",
    "        return \"'\" + str(value) + \"'\"\n",
    "    \n",
    "    \n",
    "# To be used to format possible null values in COPY FROM queries\n",
    "def format_nan_string_csv(value):\n",
    "    if pd.isna(value):\n",
    "        return 'null'\n",
    "    else:\n",
    "        return str(value) \n",
    "    \n",
    "    \n",
    "# To be used to copy tuples from a temporary CSV file to a desination table\n",
    "# Filename is the name of the temporary file, including its extension\n",
    "# Table specification is the name of the table together with the destination attributes, as in an INSERT INTO query\n",
    "def copy_to_table(filename, table_specification):\n",
    "    cursor = connection.cursor()\n",
    "    with open('./' + filename, 'r+') as f:\n",
    "        cursor.copy_expert(\"COPY \" + table_specification + \"FROM STDIN WITH NULL AS 'null' DELIMITER ','\", f)\n",
    "    connection.commit() \n",
    "    f.close()    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to an existing database\n",
    "    connection = psycopg2.connect(user=\"DBUSER\",\n",
    "                                  password=\"DBUSER_PASSWORD\",\n",
    "                                  host=\"158.110.145.70\",\n",
    "                                  port=\"5432\", \n",
    "                                  database=\"Open_Fingerprinting\",\n",
    "                                  connect_timeout=3)\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)\n",
    "   \n",
    "   \n",
    "   \n",
    " \n",
    "# Also, open SSH connection to the server, which will be used to transfer temporary CSV files\n",
    "ssh = SSHClient()\n",
    "ssh.set_missing_host_key_policy(AutoAddPolicy())\n",
    "ssh.load_system_host_keys()\n",
    "ssh.connect(hostname='158.110.145.70', \n",
    "            port = '4242',\n",
    "            username='SERVERUSER',\n",
    "            key_filename='./indoor',\n",
    "            password='SHH_KEY_PWD')\n",
    "ftp_client=ssh.open_sftp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: DEVICES.CSV\n",
    "\n",
    "if devices_exists:\n",
    "\n",
    "    print(\"Checking devices file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(devices_frame.columns) == ['device_id', 'device_model', 'device_manufacturer', 'device_type', 'notes'], \"ERROR: wrong columns in device file\"\n",
    "\n",
    "    # Are there replicated devices?\n",
    "    tmp = np.unique(devices_frame['device_id'], return_counts=True)\n",
    "    assert len(tmp[0][tmp[1] > 1]) == 0, \"ERROR: replicated device IDs have been found, \" + str(tmp[0][tmp[1] > 1])\n",
    "    \n",
    "    # Are there NULL values in device_id?\n",
    "    assert (devices_frame['device_id'].isna()).values.sum() == 0, \"ERROR: NULL values found in device_id oe\"\n",
    "    \n",
    "    # Are there inconsistent device_model, device_manufacturer, and device_type occurrences wrt NULL values\n",
    "    assert (devices_frame[['device_model', 'device_type', 'device_manufacturer']].isna()).values.sum() in [0,3], \"ERROR: inconsistent presence of NULL values found in device_manufacturer, device_model, and/or device_type\"\n",
    "\n",
    "\n",
    "    # Are the device types not among those expected by the database?\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select description from device_model_type\")\n",
    "    record = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    for raw_type in devices_frame['device_type'].unique():\n",
    "        assert raw_type in record + [np.nan], \"ERROR: Unexpected device type, \" + str(raw_type)\n",
    "\n",
    "\n",
    "    device_ids = np.unique(devices_frame['device_id'])\n",
    "\n",
    "\n",
    "    print(\"   > OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: USERS.CSV\n",
    "\n",
    "if users_exists:\n",
    "\n",
    "    print(\"Checking users file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(users_frame.columns) == ['user_id', 'username', 'type', 'notes'], \"ERROR: wrong columns in users file\"\n",
    "\n",
    "    # Are there replicated users?\n",
    "    tmp = np.unique(users_frame['user_id'], return_counts=True)\n",
    "    assert len(tmp[0][tmp[1] > 1]) == 0, \"ERROR: replicated user ids have been found, \" + str(tmp[0][tmp[1] > 1])\n",
    "    \n",
    "    tmp = np.unique(users_frame['username'], return_counts=True)\n",
    "    assert len(tmp[0][tmp[1] > 1]) == 0, \"ERROR: replicated usernames have been found, \" + str(tmp[0][tmp[1] > 1])\n",
    "    \n",
    "    \n",
    "    # Are there unexpected types of users?\n",
    "    distinct_types = np.unique(users_frame['type'])\n",
    "    for dist_type in distinct_types:\n",
    "        assert dist_type in ('online', 'trusted'), \"ERROR: unexpected user type\"\n",
    "\n",
    "        \n",
    "    # Are there NULL values in device_type or device_id?\n",
    "    assert (users_frame[['user_id', 'type']].isna()).values.sum() == 0, \"ERROR: NULL values found in either user_id or type\"\n",
    "\n",
    "\n",
    "    user_ids = np.unique(users_frame['user_id'])\n",
    "\n",
    "    print(\"   > OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking places file...\n",
      "   > OK.\n"
     ]
    }
   ],
   "source": [
    "# Verify the consistency of loaded data: PLACES.CSV\n",
    "\n",
    "if places_exists:\n",
    "\n",
    "    print(\"Checking places file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(places_frame.columns) == ['building', 'floor', 'floor_number', 'site', 'site_height', 'site_area', 'floor_height', 'floor_area', 'building_area'], \"ERROR: wrong columns in places file\"\n",
    "\n",
    "    # Are there replicated entries?\n",
    "    assert len(places_frame.drop_duplicates()) == len(places_frame), \"ERROR: replicated entries in places file have been found\"  \n",
    "    \n",
    "    # Checking information consistency\n",
    "    assert places_frame.groupby(['building']).building_area.nunique().max() <= 1, \"ERROR: building with different areas found\"\n",
    "    assert places_frame.groupby(['building', 'floor'], dropna=False).floor_number.nunique().max() <= 1, \"ERROR: floor with different floor numbers found\"\n",
    "    assert places_frame.groupby(['building', 'floor'], dropna=False).floor_height.nunique().max() <= 1, \"ERROR: floor with different heights found\"\n",
    "    assert places_frame.groupby(['building', 'floor'], dropna=False).floor_area.nunique().max() <= 1, \"ERROR: floor with different areas found\"\n",
    "    assert places_frame.groupby(['building', 'floor', 'site'], dropna=False).site_height.nunique().max() <= 1, \"ERROR: site with different heights found\"\n",
    "    assert places_frame.groupby(['building', 'floor', 'site'], dropna=False).site_area.nunique().max() <= 1, \"ERROR: site with different areas found\"\n",
    "    \n",
    "    # Are there inconsistencies related to the NULL values?\n",
    "    for ind, row in places_frame.iterrows():\n",
    "        is_null = row.isna()\n",
    "        if not(is_null['site']):\n",
    "            assert not(is_null['floor']) and not(is_null['building']), \"ERROR: non NULL site with NULL floor or building on row \" + str(ind)\n",
    "        if not(is_null['floor']):\n",
    "            assert not(is_null['floor_number']), \"ERROR: non NULL floor with NULL floor number on row \" + str(ind)\n",
    "            assert not(is_null['building']), \"ERROR: non NULL floor with NULL building on row \" + str(ind)\n",
    "        if not(is_null['site_height']) or not(is_null['site_area']):\n",
    "            assert not(is_null['site']), \"ERROR: non NULL site info with NULL site \" + str(ind)\n",
    "        if not(is_null['floor_height']) or not(is_null['floor_area']):\n",
    "            assert not(is_null['floor']), \"ERROR: non NULL floor info with NULL floor \" + str(ind)\n",
    "        assert not(is_null['building']), \"ERROR: NULL building found on row \" + str(ind)\n",
    "        \n",
    "        int(row['floor_number']) # this checks whether the floor_number can be parsed to an integer\n",
    "\n",
    "    # Aux variables needed later\n",
    "    distinct_buildings = [str(x) for x in list(places_frame[['building']].drop_duplicates().values.flatten())]\n",
    "    distinct_floors = [str(x[0]) + \"_\" + str(x[1]) for x in list(places_frame[['building', 'floor']].drop_duplicates().values)]\n",
    "    distinct_sites = [str(x[0]) + \"_\" + str(x[1]) + \"_\" + str(x[2]) for x in list(places_frame[['building', 'floor', 'site']].drop_duplicates().values)]\n",
    "\n",
    "    print(\"   > OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 322.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking tessellations file...\n",
      "   > OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the consistency of loaded data: TESSELLATIONS.CSV\n",
    "\n",
    "if tessellations_exists:\n",
    "\n",
    "    print(\"Checking tessellations file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(tessellations_frame.columns) == ['building', 'floor', 'site', 'tile', 'tessellation_type', 'coord_a_x', 'coord_a_y', 'coord_b_x', 'coord_b_y', 'coord_c_x', 'coord_c_y', 'coord_d_x', 'coord_d_y'], \"ERROR: wrong columns in tessellations file\"\n",
    "\n",
    "    # Are there replicated entries?\n",
    "    assert len(tessellations_frame.drop_duplicates()) == len(tessellations_frame), \"ERROR: replicated entries in tessellations file have been found\"\n",
    "\n",
    "    # Are there replicated tile_ids?\n",
    "    assert len(tessellations_frame[['building', 'floor', 'site', 'tile']].drop_duplicates()) == len(tessellations_frame), \"ERROR: replicated tile names (within the same floor) in tessellations file have been found\"\n",
    "\n",
    "\n",
    "    tile_id_map_type = {}\n",
    "    # Are there inconsistencies in the rows?\n",
    "    pbar = tqdm(total=len(tessellations_frame))\n",
    "    for ind, row in tessellations_frame.iterrows():\n",
    "        is_null = row.isna()\n",
    "\n",
    "        tile_id_map_type[str(row['building']) + '_' + str(row['floor']) + '_' + str(row['site']) + '_' + str(row['tile'])] = row['tessellation_type']\n",
    "\n",
    "        assert not(is_null['building']), \"ERROR: NULL building found, row \" + str(ind)\n",
    "        assert not(is_null['floor']), \"ERROR: NULL floor found, row \" + str(ind)\n",
    "        assert not(is_null['tile']), \"ERROR: NULL tile found, row \" + str(ind)\n",
    "        assert not(is_null['tessellation_type']), \"ERROR: NULL tessellation_type found, row \" + str(ind)\n",
    "        assert row['tessellation_type'] in ['logical', 'zone', 'grid', 'crowd'], \"ERROR: unexpected tessellation_type, row \" + str(ind)\n",
    "\n",
    "        # Checks for the logical tiles: each must be related to a site or floor, and at most coord_a_x and coord_a_y may be given a value \n",
    "        if row['tessellation_type'] == 'logical':\n",
    "            assert is_null[7:].sum() == 6 and (is_null[5:7].sum() in [0, 2]), \"ERROR: wrong set of coordinates given for a logical tile, row \" + str(ind)\n",
    "            assert np.sum(is_null[['building', 'floor', 'site']]) == 0 or np.sum(is_null[['building', 'floor']]) == 0, \"ERROR: logical tile not linked to non NULL building, floor, (possibly) site. Row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor'])  + \"_\" + str(row['site']) in  distinct_sites, \"ERROR: unexpected site encountered with respect to places file, row \" + str(ind)\n",
    "\n",
    "        # Checks for the zone tiles: each must be related to a site or floor, and all coord_*_x and coord_*_y must be given a value  \n",
    "        if row['tessellation_type'] == 'zone':\n",
    "            assert is_null[5:].sum() == 0, \"ERROR: wrong set of coordinates given for a zone tile, row \" + str(ind)\n",
    "            assert np.sum(is_null[['building', 'floor', 'site']]) == 0 or np.sum(is_null[['building', 'floor']]) == 0, \"ERROR: zone tile not linked to non NULL building, floor, (possibly) site. Row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor'])  + \"_\" + str(row['site']) in  distinct_sites, \"ERROR: unexpected site encountered with respect to places file, row \" + str(ind)\n",
    "\n",
    "        # Checks for the grid tiles: each must be related to a floor, and all coord_*_x and coord_*_y must be given a value \n",
    "        if row['tessellation_type'] == 'grid':\n",
    "            assert is_null[5:].sum() == 0, \"ERROR: wrong set of coordinates given for a grid tile, row \" + str(ind)\n",
    "            assert np.sum(is_null[['building', 'floor']]) == 0 and is_null['site'], \"ERROR: grid tile not correctly linked to a floor and building, row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor'])  + \"_\" + str(row['site']) in distinct_sites or str(row['building']) + \"_\" + str(row['floor']) in distinct_floors, \"ERROR: unexpected floor encountered with respect to places file, row \" + str(ind)\n",
    "            \n",
    "        # Checks for the crowd tiles:\n",
    "        if row['tessellation_type'] == 'crowd':\n",
    "            # each must be related to a floor, and not have any coordinates\n",
    "            assert is_null[5:].sum() > 0, \"ERROR: wrong set of coordinates given for a crowd tile, row \" + str(ind)\n",
    "            assert np.sum(is_null[['building', 'floor']]) == 0 and is_null['site'], \"ERROR: grid tile not correctly linked to a floor and building, row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor'])  + \"_\" + str(row['site']) in distinct_sites or str(row['building']) + \"_\" + str(row['floor']) in distinct_floors, \"ERROR: unexpected floor encountered with respect to places file, row \" + str(ind)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "            \n",
    "    # other check for crowd tessellations: a floor may contain at most one crowd tile\n",
    "    tmpcrd = tessellations_frame[tessellations_frame['tessellation_type'] == 'crowd'][['building', 'floor', 'tile']].groupby(['building', 'floor'], dropna=False).count()['tile'].values\n",
    "    if len(tmpcrd) > 1:\n",
    "        assert tmpcrd.max() == 1, \"ERROR: floor encountered with more than one crowd tile\"\n",
    "        \n",
    "\n",
    "    print(\"   > OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/9291 [00:00<00:13, 705.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking fingerprints file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9291/9291 [00:11<00:00, 820.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   > OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the consistency of loaded data: FINGERPRINTS.CSV\n",
    "\n",
    "if fingerprints_exists:\n",
    "\n",
    "    print(\"Checking fingerprints file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(fingerprints_frame.columns) == ['fingerprint_id', 'coord_x', 'coord_y', 'coord_z', 'building', 'floor', 'site', 'tile', 'user_id', 'device_id', 'epoch', 'set', 'is_radio_map', 'preceded_by', 'followed_by', 'notes']\n",
    "\n",
    "    # Are there replicated entries?\n",
    "    assert len(fingerprints_frame.drop_duplicates()) == len(fingerprints_frame), \"ERROR: replicated entries in fingerprints file have been found\"\n",
    "\n",
    "    # Are there replicated fingerprint_ids?\n",
    "    assert len(fingerprints_frame['fingerprint_id'].drop_duplicates()) == len(fingerprints_frame), \"ERROR: replicated fingerprint_ids in fingerprints file have been found\"\n",
    "\n",
    "\n",
    "    # Are there inconsistencies in the rows?\n",
    "    pbar = tqdm(total=len(fingerprints_frame))\n",
    "    for ind, row in fingerprints_frame.iterrows():\n",
    "        is_null = row.isna()\n",
    "\n",
    "        assert not(is_null[0]), \"ERROR: NULL value in fingerprint_id, row \" + str(ind)\n",
    "        assert np.sum(is_null[1:3]) in [0,2] and np.sum(is_null[1:4]) in [0,1,3], \"ERROR: wrong set of coordinates given for a fingeprint, row \" + str(ind)\n",
    "        assert is_null['device_id'] or row['device_id'] in device_ids, \"ERROR: unexpected device_id found, row \" + str(ind)\n",
    "        assert is_null['user_id'] or row['user_id'] in user_ids, \"ERROR: unexpected user_id found, row \" + str(ind)\n",
    "        assert is_null['set'] or row['set'] in ['training', 'validation', 'test'], \"ERROR: unexpected value for set, row \" + str(ind)\n",
    "        assert not is_null['is_radio_map'], \"ERROR: unspecified is_radio_map value for a given fingerprint, row \" + str(ind)\n",
    "        #assert not is_null['epoch'], \"ERROR: unspecified epoch value for a given fingerprint, row \" + str(ind)\n",
    "\n",
    "        if not(is_null['site']):\n",
    "            assert not(is_null['floor']) and not(is_null['building']), \"ERROR: non NULL site with NULL floor or building, row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor'])  + \"_\" + str(row['site']) in  distinct_sites, \"ERROR: unexpected site encountered with respect to places file, row \" + str(ind)\n",
    "        if not(is_null['floor']):\n",
    "            assert not(is_null['building']), \"ERROR: non NULL floor with NULL building, row \" + str(ind)\n",
    "            assert str(row['building']) + \"_\" + str(row['floor']) in distinct_floors, \"ERROR: unexpected floor encountered with respect to places file, row \" + str(ind)\n",
    "        if not(is_null['building']):\n",
    "            assert str(row['building']) in distinct_buildings, \"ERROR: unexpected building encountered with respect to places file, row \" + str(ind)\n",
    "        if row['is_radio_map'] == 'True':\n",
    "            assert not(is_null['tile']), \"ERROR: radio map fingerprint without a tile, row \" + str(ind) \n",
    "        if not(is_null['preceded_by']):\n",
    "            assert row['preceded_by'] in fingerprints_frame['fingerprint_id'].values, \"ERROR: could not find any references related to the preceding fingerprint id, row \" + str(ind)\n",
    "        if not(is_null['followed_by']):\n",
    "            assert row['followed_by'] in fingerprints_frame['fingerprint_id'].values, \"ERROR: could not find any references related to the following fingerprint id, row \" + str(ind)\n",
    "            \n",
    "        # Checking that tile is used in a consistent way with respect to the data defined in the file TESSELLATIONS.CSV\n",
    "        if not(is_null['tile']):\n",
    "            tile_kind = tile_id_map_type[str(row['building']) + '_' + str(row['floor']) + '_' + str(row['site']) + '_' + str(row['tile'])]\n",
    "            \n",
    "            if tile_kind in ['logical', 'zone']:\n",
    "                assert not(is_null['floor']), \"ERROR: logical or zone tile without a floor, row \" + str(ind)\n",
    "            \n",
    "            if tile_kind in ['grid', 'crowd']:\n",
    "                assert not(is_null['floor']), \"ERROR: grid or crowd tile without a floor, row \" + str(ind)\n",
    "                assert is_null['site'], \"ERROR: grid or crowd tile with a site, row \" + str(ind)\n",
    "                \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "                \n",
    "                \n",
    "    print(\"   > OK.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: ADJACENCES.CSV\n",
    "\n",
    "\n",
    "def test_if_in_map(row):\n",
    "    assert str(row['tile_1_building']) + \"_\" + str(row['tile_1_floor'])  + \"_\" + str(row['tile_1_site']) + \"_\" + str(row['tile_1_tile']) in  tile_id_map_type, \"ERROR: unexpected tile encountered with respect to tessellations file, row \" + str(ind)\n",
    "    \n",
    "\n",
    "\n",
    "non_existing = []\n",
    "\n",
    "if adjacences_exists:\n",
    "    \n",
    "    print(\"Checking adjacences file...\")\n",
    "\n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(adjacences_frame.columns) == ['tile_1_building', 'tile_1_floor', 'tile_1_site', 'tile_1_tile', 'tile_2_building', 'tile_2_floor', 'tile_2_site', 'tile_2_tile', 'walkable', 'cost']\n",
    "\n",
    "    # Are there NULL values in the file?\n",
    "    assert adjacences_frame[['tile_1_building', 'tile_1_floor', 'tile_1_tile', 'tile_2_building', 'tile_2_floor', 'tile_2_tile']].isna().sum().sum() == 0, \"ERROR: null values in the adjacences file\"\n",
    "    \n",
    "    # Are there replicated entries?\n",
    "    assert len(adjacences_frame.drop_duplicates()) == len(adjacences_frame), \"ERROR: replicated entries in adjacences file have been found\"\n",
    "\n",
    "    # Are there inconsistencies in the rows?\n",
    "    def test_if_in_map(row):\n",
    "        assert str(row['tile_1_building']) + \"_\" + str(row['tile_1_floor'])  + \"_\" + str(row['tile_1_site']) + \"_\" + str(row['tile_1_tile']) in  tile_id_map_type, \"ERROR: unexpected tile encountered with respect to tessellations file, row \" + str(ind)\n",
    "\n",
    "    _ = adjacences_frame.apply(lambda x: test_if_in_map(x), axis=1)\n",
    "    \n",
    "    \n",
    "    # Check if symmetries are satisfied\n",
    "    # Here, it makes sense to reason independently on each floor, since connections between different floors may not be symmetric (e.g., escalator)\n",
    "    for bld in fingerprints_frame['building'].drop_duplicates():\n",
    "        for flr in fingerprints_frame['floor'].drop_duplicates():\n",
    "            print('   ' + str(bld) + '   ' + str(flr))\n",
    "            # Filtering the adjacencies to keep those on the specific floor\n",
    "            adjs_red = adjacences_frame[(adjacences_frame['tile_1_building'] == bld) & (adjacences_frame['tile_1_floor'] == flr) & (adjacences_frame['tile_2_building'] == bld) & (adjacences_frame['tile_2_floor'] == flr)]\n",
    "            \n",
    "            if len(adjs_red) == 0:\n",
    "                continue\n",
    "            \n",
    "            num_rps = len(adjs_red[['tile_1_building', 'tile_1_floor', 'tile_1_site', 'tile_1_tile']].drop_duplicates())\n",
    "            aux_matrix = np.zeros((num_rps, num_rps, 2))\n",
    "                        \n",
    "            # first dimension is used to keep track of the connections (sum 1 at each observed one, hoping to obtain a symmetric matrix composed on 1s and 0s)\n",
    "            # second dimension is used in the same way but for the walkability requirement\n",
    "            # cell (i,j) tells us information regarding the link going from \"i\" to \"j\"\n",
    "\n",
    "            # first thing we have to assign a unique index to each tile\n",
    "            # let us look at the first half of the columns to do that\n",
    "            # we expect to build a map having \"num_rps\" elements\n",
    "            \n",
    "            tile_map_idx_matr = {}\n",
    "            tmp_idx = 0\n",
    "            for ind, row in adjs_red.iterrows():\n",
    "                if str(row.tile_1_building) + \"_\" + str(row.tile_1_floor) + \"_\" + str(row.tile_1_site) + \"_\" + str(row.tile_1_tile) not in tile_map_idx_matr:\n",
    "                    tile_map_idx_matr[str(row.tile_1_building) + \"_\" + str(row.tile_1_floor) + \"_\" + str(row.tile_1_site) + \"_\" + str(row.tile_1_tile)] = tmp_idx\n",
    "                    tmp_idx += 1\n",
    "\n",
    "            # now we populate \"aux_matrix\"\n",
    "            for ind, row in adjs_red.iterrows():\n",
    "                i_index = tile_map_idx_matr[str(row.tile_1_building) + \"_\" + str(row.tile_1_floor) + \"_\" + str(row.tile_1_site) + \"_\" + str(row.tile_1_tile)]\n",
    "                j_index = tile_map_idx_matr[str(row.tile_2_building) + \"_\" + str(row.tile_2_floor) + \"_\" + str(row.tile_2_site) + \"_\" + str(row.tile_2_tile)]\n",
    "                aux_matrix[i_index, j_index, 0] += 1\n",
    "                aux_matrix[i_index, j_index, 1] += int(row.walkable)\n",
    "\n",
    "\n",
    "\n",
    "            # are there repeated connections?\n",
    "            assert np.max(aux_matrix) == 1, \"Repeated connections found\"\n",
    "\n",
    "\n",
    "\n",
    "            #https://stackoverflow.com/questions/42908334/checking-if-a-matrix-is-symmetric-in-numpy\n",
    "            import scipy\n",
    "            def is_symmetric(A, tol=1e-8):\n",
    "                return scipy.linalg.norm(A-A.T, scipy.Inf) < tol;\n",
    "\n",
    "\n",
    "            # is the matrix symmetric WRT the connections?\n",
    "            check_symm_matr = aux_matrix[:, :, 0] - aux_matrix[:, :, 0].T\n",
    "            if np.max(check_symm_matr) > 0:\n",
    "                rows, cols = np.where(check_symm_matr > 0)\n",
    "                reverse_map = {}\n",
    "                for x in tile_map_idx_matr.keys():\n",
    "                    reverse_map[tile_map_idx_matr[x]] = x\n",
    "                for i in range(len(rows)):\n",
    "                    print(reverse_map[rows[i]] + \"->\" + reverse_map[cols[i]])\n",
    "\n",
    "                assert False, \"Non-symmetric connection matrix\"\n",
    "\n",
    "\n",
    "            # is the matrix symmetric WRT walkability?\n",
    "            check_symm_matr = aux_matrix[:, :, 1] - aux_matrix[:, :, 1].T\n",
    "            if np.max(check_symm_matr) > 0:\n",
    "                rows, cols = np.where(check_symm_matr > 0)\n",
    "                reverse_map = {}\n",
    "                for x in tile_map_idx_matr.keys():\n",
    "                    reverse_map[tile_map_idx_matr[x]] = x\n",
    "                for i in range(len(rows)):\n",
    "                    print(reverse_map[rows[i]] + \"->\" + reverse_map[cols[i]])\n",
    "\n",
    "                assert False, \"Non-symmetric walkability matrix\"\n",
    "\n",
    "\n",
    "\n",
    "    print(\"   > OK.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking wifi observations file...\n",
      "   > OK.\n"
     ]
    }
   ],
   "source": [
    "# Verify the consistency of loaded data: WIFI_OBS.CSV\n",
    "\n",
    "import re\n",
    "\n",
    "if wifiobs_exists:\n",
    "\n",
    "    print(\"Checking wifi observations file...\")\n",
    " \n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(wifi_obs_frame.columns)[0] == 'fingerprint_id', 'ERROR: wrong columns specified in wifi observations file'\n",
    "    assert np.any([len(re.findall(\"^AP-.+-.+\", x)) == 1 for x in list(wifi_obs_frame.columns)[1:]]), 'ERROR: wrong columns specified in wifi observations file'\n",
    "\n",
    "    assert len(wifi_obs_frame['fingerprint_id'].drop_duplicates()) == len(wifi_obs_frame), \"ERROR: replicated fingerprint_ids in wifi observations file have been found\"\n",
    "\n",
    "    assert np.max(wifi_obs_frame.iloc[:,1:].fillna(-1).values.astype('float')) <= 0, \"ERROR: wrong RSS specified in wifi observations file\"\n",
    "    \n",
    "    print(\"   > OK.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: BLUE_OBS.CSV\n",
    "\n",
    "import re\n",
    "\n",
    "if bluetoothobs_exists:\n",
    "\n",
    "    print(\"Checking bluetooth observations file...\")\n",
    " \n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(bluetooth_obs_frame.columns)[0] == 'fingerprint_id', 'ERROR: wrong columns specified in bluetooth observations file'\n",
    "    assert np.any([len(re.findall(\"^BL-.+-.+\", x)) == 1 for x in list(bluetooth_obs_frame.columns)[1:]]), 'ERROR: wrong columns specified in bluetooth observations file'\n",
    "\n",
    "    assert len(bluetooth_obs_frame['fingerprint_id'].drop_duplicates()) == len(bluetooth_obs_frame), \"ERROR: replicated fingerprint_ids in bluetooth observations file have been found\"\n",
    "\n",
    "    assert np.max(bluetooth_obs_frame.iloc[:,1:].fillna(-1).values.astype('float')) <= 0, \"ERROR: wrong RSS specified in bluetooth observations file\"\n",
    "    \n",
    "    print(\"   > OK.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: GNSS_OBS.CSV\n",
    "\n",
    "if gnssobs_exists:\n",
    "\n",
    "    print(\"Checking gnss observations file...\")\n",
    " \n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(gnss_obs_frame.columns) == ['fingerprint_id', 'latitude', 'longitude', 'elevation', 'num_satellites'], 'ERROR: wrong columns specified in gnss observations file'\n",
    "\n",
    "    assert len(gnss_obs_frame['fingerprint_id'].drop_duplicates()) == len(gnss_obs_frame), \"ERROR: replicated fingerprint_ids in gnss observations file have been found\"\n",
    "\n",
    "    # Are there NULL values in the file?\n",
    "    assert gnss_obs_frame.isna().sum().sum() == 0, \"ERROR: null values in the gnss observations file\"\n",
    "    \n",
    "    print(\"   > OK.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the consistency of loaded data: IMU_OBS.CSV\n",
    "\n",
    "if imuobs_exists:\n",
    "\n",
    "    print(\"Checking IMU observations file...\")\n",
    " \n",
    "    # Are the columns of the file as expected?\n",
    "    assert list(imu_obs_frame.columns) == ['fingerprint_id', 'epoch', 'acc_x', 'acc_y', 'acc_z', 'mag_x', 'mag_y', 'mag_z', 'gyr_x', 'gyr_y', 'gyr_z'], 'ERROR: wrong columns specified in IMU observations file'\n",
    "\n",
    "    print(\"   > OK.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################## NOW WE HAVE THE IMPORT PROCESS ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data staging area before beginning the import process\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select data_staging.truncate_tables('data_staging')\")\n",
    "record = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of all the IDs that you are importing\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select current_timestamp::text\")\n",
    "record = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "import_timestamp = record[0][0]\n",
    "\n",
    "\n",
    "\n",
    "file_imported = open('./temp_imported.csv', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source \n",
    "cursor = connection.cursor()\n",
    "string_insert = \"INSERT INTO data_staging.data_source(name, url, notes) VALUES ('\" + str(dataset_name) + \"', \" + str(format_nan_string(dataset_url)) + \", \" + str(format_nan_string(dataset_notes)) + \")\"\n",
    "string_insert = string_insert.replace(\"'NULL'\", \"null\")\n",
    "cursor.execute(string_insert)\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select id from data_staging.data_source\")\n",
    "dsid = cursor.fetchall()[0][0]\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "file_imported.write(str(dsid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"data_source\\n\")\n",
    "\n",
    "# Is there another data source with the same name or url already stored in the database?\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select name, url from public.data_source\")\n",
    "res = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "if len(res) > 0:\n",
    "    #if dataset_url != 'NULL':\n",
    "    #    assert not(dataset_name in np.asarray(res)[:,0] or dataset_url in np.asarray(res)[:,1]), \"ERROR: dataset appears to be already present in the database public schema\"\n",
    "   # else:\n",
    "        assert not(dataset_name in np.asarray(res)[:,0]), \"ERROR: dataset appears to be already present in the database public schema\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "\n",
    "if devices_exists:\n",
    "    \n",
    "    unique_devices = devices_frame[['device_model', 'device_manufacturer', 'device_type']].drop_duplicates()\n",
    "\n",
    "        \n",
    "    # Insert into device_model\n",
    "    for _, dev in unique_devices.iterrows():\n",
    "        if np.sum(dev.isna()) == 0:\n",
    "            # To insert in case which the device moodel does not exist\n",
    "            string_insert = \"INSERT INTO data_staging.device_model(name, manufacturer, type_id) \\\n",
    "                                SELECT '\" + str(dev[0]) + \"','\" + str(dev[1]) + \"', device_model_type.id \\\n",
    "                                FROM device_model_type \\\n",
    "                                WHERE device_model_type.description = '\" + str(dev[2]) + \"'\\\n",
    "                                        AND NOT EXISTS (SELECT * \\\n",
    "                                                        FROM public.device_model \\\n",
    "                                                        WHERE device_model.name = '\" + str(dev[0]) + \"' and device_model.manufacturer = '\" + str(dev[1]) + \"');\"\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(string_insert)\n",
    "            connection.commit() \n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Generating new ids for the single devices\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    string_insert = \"INSERT INTO data_staging.generated_ids(dummy_field) VALUES \"\n",
    "    for b_id in np.unique(devices_frame['device_id']):\n",
    "        string_insert += \"(null), \"\n",
    "    string_insert = string_insert[:-2] + \";\"\n",
    "    cursor.execute(string_insert)\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Inserting the single devices\n",
    "    device_code_map_id = {} # maps that keeps the correspondence between the device code in the dataset and its newly generated ID in the database\n",
    "    for idx, ind_row in enumerate(devices_frame.iterrows()):\n",
    "        row = ind_row[1]\n",
    "        device_code_map_id[row.device_id] = generated_ids[idx]\n",
    "        string_insert = \"INSERT INTO data_staging.device(code, data_source_id, id, device_model_id, notes) \\\n",
    "                            SELECT '\" + str(row.device_id)  + \"', data_source.id, \" + str(generated_ids[idx]) + \", COALESCE(ds_model.id, pb_model.id), \" + format_nan_string(row.notes) + \"   \\\n",
    "                            FROM data_staging.data_source \\\n",
    "                                    LEFT OUTER JOIN data_staging.device_model AS ds_model ON ds_model.manufacturer = '\" + str(row.device_manufacturer) + \"' AND ds_model.name = '\" + str(row.device_model) + \"'\\\n",
    "                                    LEFT OUTER JOIN device_model_type AS ds_model_type ON ds_model.type_id = ds_model_type.id AND ds_model_type.description = '\" + str(row.device_type) + \"'\\\n",
    "                                    LEFT OUTER JOIN device_model AS pb_model ON pb_model.manufacturer = '\" + str(row.device_manufacturer) + \"' AND pb_model.name = '\" + str(row.device_model) + \"'\\\n",
    "                                    LEFT OUTER JOIN device_model_type AS pb_model_type ON pb_model.type_id = pb_model_type.id AND pb_model_type.description = '\" + str(row.device_type) + \"';\" \n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(string_insert)\n",
    "        connection.commit() \n",
    "        cursor.close()\n",
    "        \n",
    "        \n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"device\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users\n",
    "\n",
    "\n",
    "if users_exists:\n",
    "\n",
    "    # Generating new ids for the single users\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    string_insert = \"INSERT INTO data_staging.generated_ids(dummy_field) VALUES \"\n",
    "    for i in range(len(users_frame)):\n",
    "        string_insert += \"(null), \"\n",
    "    string_insert = string_insert[:-2] + \";\"\n",
    "    cursor.execute(string_insert)\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Inserting the single users\n",
    "    user_code_map_id = {} # maps that keeps the correspondence between the user code in the dataset and its newly generated ID in the database\n",
    "    for idx, indrow in enumerate(users_frame.iterrows()):\n",
    "        row = indrow[1]\n",
    "        user_code_map_id[row.user_id] = generated_ids[idx]\n",
    "        cursor = connection.cursor()\n",
    "        if row.type == 'online':\n",
    "            u_type = 1\n",
    "        else:\n",
    "            u_type = 2\n",
    "        \n",
    "        string_insert = \"INSERT INTO data_staging.user(id, code, username, type_id, data_source_id, notes) \\\n",
    "                        SELECT \" + str(generated_ids[idx]) + \", '\" + str(row.user_id) + \"', \" + format_nan_string(row.username) + \", \" + str(u_type) + \", data_source.id, \" + format_nan_string(row.notes) + \"\\\n",
    "                        FROM data_staging.data_source;\"\n",
    "        string_insert = string_insert.replace(\"'NULL'\", \"null\")\n",
    "        \n",
    "        cursor.execute(string_insert)\n",
    "        connection.commit() \n",
    "        cursor.close()\n",
    "\n",
    "        \n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"user\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building (+ Place, + place_data_source)\n",
    "\n",
    "# Generating new ids\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "cursor = connection.cursor()\n",
    "string_insert = \"INSERT INTO data_staging.generated_ids(dummy_field) VALUES \"\n",
    "for b_id in np.unique(places_frame['building']):\n",
    "    string_insert += \"(null), \"\n",
    "string_insert = string_insert[:-2] + \";\"\n",
    "cursor.execute(string_insert)\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "\n",
    "# Inserting into place\n",
    "building_map_id = {} # maps that keeps the correspondence between the building code in the dataset and its newly generated ID in the database\n",
    "cursor = connection.cursor()\n",
    "string_insert = \"INSERT INTO data_staging.place(id, name) VALUES \"\n",
    "for idx, b_id in enumerate(np.unique(places_frame['building'])):\n",
    "    string_insert += \"(\" + str(generated_ids[idx]) + \",'\" + str(b_id) + \"'), \"\n",
    "    building_map_id[b_id] = generated_ids[idx]\n",
    "string_insert = string_insert[:-2] + \";\"\n",
    "cursor.execute(string_insert)\n",
    "connection.commit() \n",
    "cursor.close()                                \n",
    "\n",
    "                                \n",
    "# Inserting into building\n",
    "cursor = connection.cursor()\n",
    "string_insert = \"INSERT INTO data_staging.building(place_id, area) VALUES \"\n",
    "for idx, bldinfo in enumerate(places_frame[['building', 'building_area']].drop_duplicates().iterrows()):\n",
    "    string_insert += \"(\" + str(generated_ids[idx]) + \",\" + format_nan_string(bldinfo[1].building_area) + \"), \"\n",
    "string_insert = string_insert[:-2] + \";\"\n",
    "cursor.execute(string_insert)\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "\n",
    "# Inserting into place_data_source\n",
    "cursor = connection.cursor()\n",
    "string_insert = \"INSERT INTO data_staging.place_data_source(data_source_id, place_id) \\\n",
    "                    SELECT data_source.id, building.place_id FROM data_staging.data_source JOIN data_staging.building ON 1=1;\"\n",
    "cursor.execute(string_insert)\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "\n",
    "for insid in generated_ids:\n",
    "    file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"building\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floor (+ Place, + place_data_source, + contains) including floor above and below relationships\n",
    "\n",
    "bld_flr_names = places_frame[np.logical_not(pd.isna(places_frame.floor))][['building', 'floor', 'floor_number', 'floor_height', 'floor_area']].drop_duplicates()\n",
    "\n",
    "\n",
    "if len(bld_flr_names) > 0: # if there are some floors to import\n",
    "\n",
    "    # Generating new ids\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    string_insert = \"INSERT INTO data_staging.generated_ids(dummy_field) VALUES \"\n",
    "    for i in range(len(bld_flr_names)):\n",
    "        string_insert += \"(null), \"\n",
    "    string_insert = string_insert[:-2] + \";\"\n",
    "    cursor.execute(string_insert)\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bldflr_map_id = {} # maps that keeps the correspondence between the \"floor_building\" code in the dataset and its newly generated ID in the database\n",
    "    bldflr_map_number = {} # maps that tells us the number (vertical ordering) of each floor\n",
    "    for idx, bld_flr in enumerate(bld_flr_names.iterrows()):\n",
    "        bld = bld_flr[1].building\n",
    "        flr = bld_flr[1].floor\n",
    "        floor_height = bld_flr[1].floor_height\n",
    "        floor_area = bld_flr[1].floor_area\n",
    "        bldflr_map_id[str(bld) + \"_\" + str(flr)] = generated_ids[idx]\n",
    "        bldflr_map_number[str(bld) + \"_\" + str(flr)] = bld_flr[1].floor_number\n",
    "\n",
    "        # Insert into place\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"INSERT INTO data_staging.place(id, name) VALUES (\" + str(generated_ids[idx]) + \",'\" + str(flr) + \"');\")\n",
    "        connection.commit() \n",
    "        cursor.close()\n",
    "\n",
    "        # Insert into place_data_source\n",
    "        cursor = connection.cursor()\n",
    "        string_insert = \"INSERT INTO data_staging.place_data_source(data_source_id, place_id) SELECT id, \" + str(generated_ids[idx]) + \" from data_staging.data_source;\"\n",
    "        cursor.execute(string_insert)\n",
    "        connection.commit() \n",
    "        cursor.close()\n",
    "\n",
    "        # Update contains\n",
    "        cursor = connection.cursor()\n",
    "        string_insert = \"INSERT INTO data_staging.contains(container_place_id, contained_place_id) VALUES (\" + str(building_map_id[bld]) + \",\" + str(generated_ids[idx]) + \");\"\n",
    "        cursor.execute(string_insert)\n",
    "        connection.commit() \n",
    "        cursor.close()    \n",
    "\n",
    "        # Insert into floor\n",
    "        cursor = connection.cursor()\n",
    "        string_insert = \"INSERT INTO data_staging.floor(place_id, area, height) VALUES (\" + str(generated_ids[idx]) + \",\" + format_nan_string(floor_area) + \",\" + format_nan_string(floor_height) + \");\"\n",
    "        cursor.execute(string_insert)\n",
    "        connection.commit() \n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "    # Update above/below floor relationships\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select bldplace.name as bldname, flrplace.name as flrname, flrplace.id as flrid \\\n",
    "                    from data_staging.place as bldplace \\\n",
    "                            join data_staging.building on building.place_id = bldplace.id \\\n",
    "                            join data_staging.contains on container_place_id = building.place_id  \\\n",
    "                            join data_staging.place as flrplace on flrplace.id = contained_place_id \\\n",
    "                    order by flrplace.name::text, bldplace.name::text;\")\n",
    "    records = np.asarray(cursor.fetchall()) # building name, floor name, floor id\n",
    "    connection.commit() \n",
    "    cursor.close()    \n",
    "\n",
    "\n",
    "    bld_flrnumber_map_flr = {} # given a building name and a floor number, it gives me the floor name\n",
    "    for key in bldflr_map_number.keys():\n",
    "        bld = key.split(\"_\")[0]\n",
    "        flr = key.split(\"_\")[1]\n",
    "        num = bldflr_map_number[key]\n",
    "        bld_flrnumber_map_flr[bld + \"_\" + str(num)] = str(flr)\n",
    "\n",
    "\n",
    "    for bldname in np.unique(records[:,0]):\n",
    "        records_bld = records[records[:,0] == bldname]\n",
    "        if len(records_bld) > 1: # if there is more than one floor in the considered building\n",
    "            bld_floor_numbers = np.unique([int(bldflr_map_number[str(records_bld[i, 0]) + \"_\" + str(records_bld[i, 1])]) for i in range(len(records_bld))])\n",
    "            for bld_flr_flrid in records_bld:\n",
    "                flr_number = int(bldflr_map_number[str(bld_flr_flrid[0] + \"_\" + bld_flr_flrid[1])])            \n",
    "                if flr_number == bld_floor_numbers[0]:\n",
    "                    flr_number_above = bld_floor_numbers[np.where(bld_floor_numbers > flr_number)[0][0]]\n",
    "                    string_update = \"UPDATE data_staging.floor SET below_of_floor_place_id = \" + str(bldflr_map_id[bldname + \"_\" + bld_flrnumber_map_flr[str(bldname) + \"_\" + str(flr_number_above)]]) + \" WHERE place_id = \" +  str(bld_flr_flrid[2]) + \";\"\n",
    "                elif flr_number == bld_floor_numbers[-1]:\n",
    "                    flr_number_below = bld_floor_numbers[np.where(bld_floor_numbers < flr_number)[0][-1]]\n",
    "                    string_update = \"UPDATE data_staging.floor SET above_of_floor_place_id = \" + str(bldflr_map_id[bldname + \"_\" + bld_flrnumber_map_flr[str(bldname) + \"_\" + str(flr_number_below)]])  + \" WHERE place_id = \" +  str(bld_flr_flrid[2]) + \";\"\n",
    "                else:\n",
    "                    flr_number_above = bld_floor_numbers[np.where(bld_floor_numbers > flr_number)[0][0]]\n",
    "                    flr_number_below = bld_floor_numbers[np.where(bld_floor_numbers < flr_number)[0][-1]]\n",
    "                    string_update = \"UPDATE data_staging.floor SET below_of_floor_place_id = \" + str(bldflr_map_id[bldname + \"_\" + bld_flrnumber_map_flr[str(bldname) + \"_\" + str(flr_number_above)]]) + \\\n",
    "                                    \", above_of_floor_place_id = \" + str(bldflr_map_id[bldname + \"_\" + bld_flrnumber_map_flr[str(bldname) + \"_\" + str(flr_number_below)]]) + \\\n",
    "                                    \" WHERE place_id = \" +  str(bld_flr_flrid[2]) + \";\"\n",
    "                cursor = connection.cursor()\n",
    "                cursor.execute(string_update)\n",
    "                connection.commit() \n",
    "                cursor.close()    \n",
    "\n",
    "\n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"floor\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site (+ Place, + place_data_source, + contains) \n",
    "\n",
    "bld_flr_site_names = places_frame[(np.logical_not(pd.isna(places_frame.floor))) & (np.logical_not(pd.isna(places_frame.site)))][['building', 'floor', 'site', 'site_height', 'site_area']].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "if len(bld_flr_site_names) > 0: # if there are some sites to import\n",
    "\n",
    "\n",
    "    # Generating new ids\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for i in range(len(bld_flr_site_names)):\n",
    "            file.write(\"null\\n\")\n",
    "    copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "    \n",
    "        \n",
    "    file_place = open('./temp_place.csv', 'w')\n",
    "    file_place_data_source = open('./temp_place_data_source.csv', 'w')\n",
    "    file_contains = open('./temp_contains.csv', 'w')\n",
    "    file_site = open('./temp_site.csv', 'w')\n",
    "    \n",
    "    bldflrsite_map_id = {} # maps that keeps the correspondence between the \"building_floor_site\" code in the dataset and its newly generated ID in the database\n",
    "    for idx, flr_bld_site in enumerate(bld_flr_site_names.iterrows()):\n",
    "        bld = flr_bld_site[1].building\n",
    "        flr = flr_bld_site[1].floor\n",
    "        site = flr_bld_site[1].site\n",
    "        site_height = flr_bld_site[1].site_height\n",
    "        site_area = flr_bld_site[1].site_area\n",
    "\n",
    "        bldflrsite_map_id[str(bld) + \"_\" + str(flr) + \"_\" + str(site)] = generated_ids[idx]\n",
    "\n",
    "\n",
    "        # Insert into place\n",
    "        file_place.write(str(generated_ids[idx]) + \",\" + str(site) + \"\\n\")\n",
    "\n",
    "        # Insert into place_data_source\n",
    "        file_place_data_source.write(str(dsid) + \",\" + str(generated_ids[idx]) + \"\\n\")\n",
    "\n",
    "\n",
    "        # Insert into contains\n",
    "        file_contains.write(str(bldflr_map_id[str(bld) + \"_\" + str(flr)]) + \",\" + str(generated_ids[idx]) + \"\\n\")\n",
    "\n",
    "\n",
    "        # Insert into site\n",
    "        file_site.write(str(generated_ids[idx]) + \",\" + format_nan_string_csv(site_area) + \",\" + format_nan_string_csv(site_height) + \"\\n\")\n",
    "        \n",
    "    \n",
    "    file_place.close()\n",
    "    file_place_data_source.close()\n",
    "    file_contains.close()\n",
    "    file_site.close()       \n",
    "\n",
    "    copy_to_table(\"temp_place.csv\", \"data_staging.place(id, name)\")   \n",
    "    copy_to_table(\"temp_place_data_source.csv\", \"data_staging.place_data_source(data_source_id, place_id)\") \n",
    "    copy_to_table(\"temp_contains.csv\", \"data_staging.contains(container_place_id, contained_place_id)\") \n",
    "    copy_to_table(\"temp_site.csv\", \"data_staging.site(place_id, area, height)\") \n",
    "\n",
    "\n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"site\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tessellation\n",
    "\n",
    "bldflrtype_map_tessid = {}\n",
    "if tessellations_exists:\n",
    "    \n",
    "    distinct_tass = tessellations_frame[['building', 'floor', 'tessellation_type']].drop_duplicates()\n",
    "    \n",
    "    # Generating new ids\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for i in range(len(distinct_tass)):\n",
    "            file.write(\"null\\n\")\n",
    "    copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    \n",
    "\n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for ir, row in enumerate(distinct_tass.iterrows()):\n",
    "            file.write(str(generated_ids[ir]) + \",\" + str(bldflr_map_id[str(row[1]['building']) + \"_\" + str(row[1]['floor'])]) + \",\" + str(dsid) + \",\" + str(row[1]['tessellation_type']) +\"\\n\")\n",
    "            bldflrtype_map_tessid[str(row[1]['building']) + \"_\" + str(row[1]['floor']) + \"_\" + str(row[1]['tessellation_type'])] = generated_ids[ir]\n",
    "            \n",
    "    copy_to_table(\"temp.csv\", \"data_staging.tessellation(id, floor_place_id, data_source_id, type)\")        \n",
    "            \n",
    "        \n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"tessellation\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 540.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tile\n",
    "\n",
    "\n",
    "if tessellations_exists:\n",
    "\n",
    "    # Generating new ids\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for i in range(len(tessellations_frame)):\n",
    "            file.write(\"null\\n\")\n",
    "    copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "    \n",
    "    tile_map_id = {} # maps that keeps the correspondence between the \"tile\" in the dataset (here identified by building, floor, site, tile) and its newly generated ID in the database\n",
    "    to_calculate_logical = {} # map that holds the ids -> [] of the logical tiles for which I will have to calculate the centroid from the associated fingerprints\n",
    "    pbar = tqdm(total=len(tessellations_frame))\n",
    "    \n",
    "    \n",
    "    file_place = open('./temp_place.csv', 'w')\n",
    "    file_place_data_source = open('./temp_place_data_source.csv', 'w')\n",
    "    file_contains = open('./temp_contains.csv', 'w')\n",
    "    file_tile = open('./temp_tile.csv', 'w')\n",
    "    \n",
    "    for idx, tileinfo in enumerate(tessellations_frame.iterrows()):\n",
    "\n",
    "        is_null = tileinfo[1].isna()\n",
    "\n",
    "        bld = tileinfo[1][0]\n",
    "        flr = tileinfo[1][1]\n",
    "        site = tileinfo[1][2]\n",
    "        tile = tileinfo[1][3]\n",
    "        t_type = tileinfo[1][4]\n",
    "\n",
    "        x_a = tileinfo[1][5]\n",
    "        y_a = tileinfo[1][6]\n",
    "\n",
    "        x_b = tileinfo[1][7]\n",
    "        y_b = tileinfo[1][8]\n",
    "\n",
    "        x_c = tileinfo[1][9]\n",
    "        y_c = tileinfo[1][10]\n",
    "\n",
    "        x_d = tileinfo[1][11]\n",
    "        y_d = tileinfo[1][12]\n",
    "\n",
    "        tile_map_id[str(bld) + \"_\" + str(flr) + \"_\" + str(site) + \"_\" + str(tile)] = generated_ids[idx]\n",
    "\n",
    "\n",
    "        # Insert into place\n",
    "        file_place.write(str(generated_ids[idx]) + \",\" + str(tile) + \"\\n\")\n",
    "\n",
    "\n",
    "        # Insert into place_data_source\n",
    "        file_place_data_source.write(str(dsid) + \",\" + str(generated_ids[idx]) + \"\\n\")\n",
    "\n",
    "\n",
    "        # Insert into contains\n",
    "        if is_null[2]: # if site is null\n",
    "            # if the tile is contained in a floor\n",
    "            file_contains.write(str(bldflr_map_id[str(bld) + \"_\" + str(flr)]) + \",\" + str(generated_ids[idx]) + \"\\n\")\n",
    "        else:\n",
    "            # else, if the tile is contained in a site\n",
    "            file_contains.write(str(bldflrsite_map_id[str(bld) + \"_\" + str(flr) + \"_\" + str(site)]) + \",\" + str(generated_ids[idx]) + \"\\n\")\n",
    "\n",
    "            \n",
    "        # Insert into tile\n",
    "        if t_type == 'logical':\n",
    "            if is_null[5] and is_null[6]:\n",
    "                # if I don't have the coordinates, then I will have to derive them from the average of the associated fingerprints\n",
    "                to_calculate_logical[generated_ids[idx]] = []\n",
    "                file_tile.write(str(generated_ids[idx]) + \",null,null,null,null,null,null,null,null,logical,\" + str(bldflrtype_map_tessid[str(bld) + \"_\" + str(flr) + \"_logical\"]) + \"\\n\")\n",
    "            else:\n",
    "                file_tile.write(str(generated_ids[idx]) + \",\" + str(x_a) + \",\" + str(y_a) + \",null,null,null,null,null,null,logical,\" + str(bldflrtype_map_tessid[str(bld) + \"_\" + str(flr) + \"_logical\"]) + \"\\n\")\n",
    "\n",
    "        elif t_type in ('zone', 'grid'):\n",
    "            file_tile.write(str(generated_ids[idx]) + \",\" + str(x_a) + \",\" + str(y_a) + \",\" + str(x_b) + \",\" + str(y_b) + \",\" + str(x_c) + \",\" + str(y_c) + \",\" + str(x_d) + \",\" + str(y_d) + \",\" + str(t_type) + \",\" + str(bldflrtype_map_tessid[str(bld) + \"_\" + str(flr) + \"_\" + str(t_type)]) + \"\\n\")\n",
    "        \n",
    "        else:\n",
    "            # crowd\n",
    "            file_tile.write(str(generated_ids[idx]) + \",null,null,null,null,null,null,null,null,crowd,\" + str(bldflrtype_map_tessid[str(bld) + \"_\" + str(flr) + \"_crowd\"]) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        file_imported.write(str(generated_ids[idx]) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"tile\\n\")\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    pbar.close()   \n",
    "\n",
    "        \n",
    "    file_place.close()\n",
    "    file_place_data_source.close()\n",
    "    file_contains.close()\n",
    "    file_tile.close()       \n",
    "\n",
    "    copy_to_table(\"temp_place.csv\", \"data_staging.place(id, name)\")   \n",
    "    copy_to_table(\"temp_place_data_source.csv\", \"data_staging.place_data_source(data_source_id, place_id)\") \n",
    "    copy_to_table(\"temp_contains.csv\", \"data_staging.contains(container_place_id, contained_place_id)\") \n",
    "    copy_to_table(\"temp_tile.csv\", \"data_staging.tile(place_id, coordinate_a_x, coordinate_a_y, coordinate_b_x, coordinate_b_y, coordinate_c_x, coordinate_c_y, coordinate_d_x, coordinate_d_y, type, tessellation_id)\") \n",
    "\n",
    "    \n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"tile\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adjacent_to_tile\n",
    "\n",
    "if adjacences_exists: \n",
    "    \n",
    "    with open('./temp_adjacences.csv', 'w') as file:\n",
    "        pbar = tqdm(total=len(adjacences_frame))\n",
    "        for ind, row in adjacences_frame.iterrows():\n",
    "            tile_1_id = tile_map_id[str(row.tile_1_building) + \"_\" + str(row.tile_1_floor) + \"_\" + str(row.tile_1_site) + \"_\" + str(row.tile_1_tile)]\n",
    "            tile_2_id = tile_map_id[str(row.tile_2_building) + \"_\" + str(row.tile_2_floor) + \"_\" + str(row.tile_2_site) + \"_\" + str(row.tile_2_tile)]\n",
    "            walkable_val = 'null' if pd.isna(row.walkable) else str(bool(int(row.walkable)))\n",
    "            file.write(str(tile_1_id) + \",\" + str(tile_2_id) + \",\" + walkable_val + \",\" + format_nan_string_csv(row.cost) + \"\\n\")\n",
    "            pbar.update(1)\n",
    "        pbar.close()   \n",
    "            \n",
    "    copy_to_table(\"temp_adjacences.csv\", \"data_staging.adjacent_to_tile(tile_1_place_id, tile_2_place_id, walkable, cost)\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9291/9291 [00:12<00:00, 759.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fingerprints\n",
    "# Here I should also update the coordinates for the logical tiles that have their id in to_calculate_logical, considering only fingerprints that are both training and radio-map\n",
    "\n",
    "# Generating new ids\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "with open('./temp.csv', 'w') as file:\n",
    "    for i in range(len(fingerprints_frame)):\n",
    "        file.write(\"null\\n\")\n",
    "copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "file_fingerprint = open('./temp_fingerprint.csv', 'w')\n",
    "file_gtinfo = open('./temp_gtinfo.csv', 'w')\n",
    "\n",
    "fingerprint_code_map_id = {}\n",
    "fingerprint_map_preceding_code = {}\n",
    "fingerprint_map_following_code = {}\n",
    "pbar = tqdm(total=len(fingerprints_frame))\n",
    "for idx, fing in enumerate(fingerprints_frame.iterrows()):\n",
    "    is_null = fing[1].isna()\n",
    "    \n",
    "    fingerprint_code_map_id[str(fing[1].fingerprint_id)] = generated_ids[idx]\n",
    "    \n",
    "    if not(is_null.preceded_by):\n",
    "        fingerprint_map_preceding_code[generated_ids[idx]] = str(fing[1].preceded_by)\n",
    "    if not(is_null.followed_by):\n",
    "        fingerprint_map_following_code[generated_ids[idx]] = str(fing[1].followed_by)\n",
    "        \n",
    "        \n",
    "    file_fingerprint.write(str(generated_ids[idx]) + \",\" \\\n",
    "                             + str(fing[1].fingerprint_id) + \",\" \\\n",
    "                             + str(dsid) + \",\" \\\n",
    "                             + ('null' if is_null.epoch else str(datetime.datetime.fromtimestamp(int(fing[1].epoch)).strftime('%Y-%m-%d %H:%M:%S')))  + \",\" \\\n",
    "                             + ('null' if is_null.coord_x or str(fing[1].is_radio_map) == \"False\" else str(fing[1].coord_x))  + \",\" \\\n",
    "                             + ('null' if is_null.coord_y or str(fing[1].is_radio_map) == \"False\" else str(fing[1].coord_y))  + \",\" \\\n",
    "                             + ('null' if is_null.coord_z or str(fing[1].is_radio_map) == \"False\" else str(fing[1].coord_z))  + \",\" \\\n",
    "                             +  \"null,\" \\\n",
    "                             +  \"null,\" \\\n",
    "                             + ('null' if is_null.user_id else str(user_code_map_id[str(fing[1].user_id)]))  + \",\" \\\n",
    "                             + ('null' if is_null.device_id else str(device_code_map_id[str(fing[1].device_id)]))  + \",\" \\\n",
    "                             + str(fing[1].is_radio_map) + \",\" \\\n",
    "                             + ('null' if str(fing[1].is_radio_map) == \"False\" else str(tile_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor) + \"_\" + str(fing[1].site) + \"_\" + str(fing[1].tile)])) + \",\" \\\n",
    "                             + ('null' if is_null.set else str(fing[1].set)) + \",\" \\\n",
    "                             + ('null' if is_null.notes else str(fing[1].notes)) + \"\\n\")\n",
    "    \n",
    "    file_gtinfo.write(str(generated_ids[idx]) + \",\" \\\n",
    "                         + ('null' if is_null.coord_x else str(fing[1].coord_x))  + \",\" \\\n",
    "                         + ('null' if is_null.coord_y else str(fing[1].coord_y))  + \",\" \\\n",
    "                         + ('null' if is_null.coord_z else str(fing[1].coord_z))  + \",\" \\\n",
    "                         + ('null' if is_null.tile  else str(tile_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor) + \"_\" + str(fing[1].site) + \"_\" + str(fing[1].tile)]))  + \",\" \\\n",
    "                         + ('null' if is_null.site  else str(bldflrsite_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor) + \"_\" + str(fing[1].site)]))  + \",\" \\\n",
    "                         + ('null' if is_null.floor  else str(bldflr_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor)]))  + \",\" \\\n",
    "                         + ('null' if is_null.building  else str(building_map_id[str(fing[1].building)])) + \"\\n\")\n",
    "  \n",
    "\n",
    "    if str(fing[1].set) == 'training' and str(fing[1].is_radio_map) == 'True':\n",
    "        if tile_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor) + \"_\" + str(fing[1].site) + \"_\" + str(fing[1].tile)] in to_calculate_logical:\n",
    "            to_calculate_logical[tile_map_id[str(fing[1].building) + \"_\" + str(fing[1].floor) + \"_\" + str(fing[1].site) + \"_\" + str(fing[1].tile)]].append([fing[1].coord_x, fing[1].coord_y, fing[1].coord_z])\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()                   \n",
    "       \n",
    "    \n",
    "file_fingerprint.close()\n",
    "file_gtinfo.close()\n",
    "\n",
    "\n",
    "                           \n",
    "copy_to_table(\"temp_fingerprint.csv\", \"data_staging.fingerprint(id, code, data_source_id, timestamp, coordinate_x, coordinate_y, coordinate_z, preceded_by_fingerprint_id, followed_by_fingerprint_id, user_id, device_id, is_radio_map, acquired_at_tile_place_id, ml_purpose, notes)\")   \n",
    "copy_to_table(\"temp_gtinfo.csv\", \"data_staging.ground_truth_info(fingerprint_id, coordinate_x, coordinate_y, coordinate_z, tile_place_id, site_place_id, floor_place_id, building_place_id)\") \n",
    "\n",
    "\n",
    "\n",
    "                           \n",
    "                           \n",
    "# Updating the trajectory relationship between fingerprints\n",
    "for idt in fingerprint_map_preceding_code.keys():\n",
    "    string_query = \"UPDATE data_staging.fingerprint SET \\\n",
    "                        preceded_by_fingerprint_id = \" + str(fingerprint_code_map_id[fingerprint_map_preceding_code[idt]]) + \"\\\n",
    "                    WHERE id = \" + str(idt) + \";\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(string_query)\n",
    "    connection.commit() \n",
    "    cursor.close()   \n",
    "    \n",
    "for idt in fingerprint_map_following_code.keys():\n",
    "    string_query = \"UPDATE data_staging.fingerprint SET \\\n",
    "                        followed_by_fingerprint_id = \" + str(fingerprint_code_map_id[fingerprint_map_following_code[idt]]) + \"\\\n",
    "                    WHERE id = \" + str(idt) + \";\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(string_query)\n",
    "    connection.commit() \n",
    "    cursor.close()   \n",
    "\n",
    "    \n",
    "\n",
    "# Updating the logical tiles coordinates\n",
    "for idt in to_calculate_logical:\n",
    "    string_query = \"WITH avg_coords AS ( \\\n",
    "                        SELECT avg(fingerprint.coordinate_x) AS avg_x, avg(fingerprint.coordinate_y) as avg_y \\\n",
    "                        FROM data_staging.fingerprint \\\n",
    "                        WHERE acquired_at_tile_place_id = \" + str(idt) + \" )  \\\n",
    "                    UPDATE data_staging.tile SET  \\\n",
    "                        coordinate_a_x =  avg_coords.avg_x,  \\\n",
    "                        coordinate_a_y =  avg_coords.avg_y \\\n",
    "                    FROM avg_coords \\\n",
    "                    WHERE tile.place_id = \" + str(idt) + \";\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(string_query)\n",
    "    connection.commit() \n",
    "    cursor.close()   \n",
    "\n",
    "\n",
    "\n",
    "for insid in generated_ids:\n",
    "    file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"fingerprint\\n\")\n",
    "                           \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7442091/7442091 [02:03<00:00, 60109.03it/s] \n"
     ]
    }
   ],
   "source": [
    "# Access Points and Wi-Fi observations\n",
    "\n",
    "if wifiobs_exists:\n",
    "\n",
    "    # Generating new ids \n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for i in range(len([x for x in wifi_obs_frame.columns if 'AP-' in x])):\n",
    "            file.write(\"null\\n\")\n",
    "    copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    file_ap = open('./temp_ap.csv', 'w')\n",
    "    file_apds = open('./temp_apds.csv', 'w')\n",
    "    file_obs = open('./temp_obs.csv', 'w')\n",
    "    file_detect = open('./temp_detect.csv', 'w')    \n",
    "    \n",
    "    \n",
    "    # ap and ap_data_source\n",
    "    ap_map_id = {} # maps that keeps the correspondence between the ap name in the dataset and its newly generated ID in the database\n",
    "    for idx, apname in enumerate([x for x in wifi_obs_frame.columns if 'AP-' in x]):\n",
    "        macvalue = 'null' if str(apname.split('-')[2]) == 'NULL' else str(apname.split('-')[2]) \n",
    "        ap_map_id[apname] = generated_ids[idx]\n",
    "        file_ap.write(str(generated_ids[idx]) + \",\" + str(apname.split('-')[1]) + \",\" + macvalue + \"\\n\")\n",
    "        file_apds.write(str(generated_ids[idx]) + \",\" + str(dsid) + \"\\n\")\n",
    "\n",
    "    \n",
    "    # Wi-Fi observation and AP detection\n",
    "\n",
    "    pbar = tqdm(total=len(wifi_obs_frame)*len([x for x in wifi_obs_frame.columns if 'AP-' in x]))\n",
    "    for _, fing in wifi_obs_frame.iterrows():\n",
    "        file_obs.write(str(fingerprint_code_map_id[str(fing.fingerprint_id)]) + \"\\n\")\n",
    "        for apname in [x for x in wifi_obs_frame.columns if 'AP-' in x]:\n",
    "            if not pd.isna(fing[apname]):\n",
    "                file_detect.write(str(ap_map_id[apname]) + \",\" + str(fingerprint_code_map_id[str(fing.fingerprint_id)]) + \",\" + str(fing[apname]) + \"\\n\")     \n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "file_ap.close()\n",
    "file_apds.close()\n",
    "file_obs.close()\n",
    "file_detect.close()\n",
    "\n",
    "\n",
    "copy_to_table(\"temp_ap.csv\", \"data_staging.ap(id, code, mac)\")   \n",
    "copy_to_table(\"temp_apds.csv\", \"data_staging.ap_data_source(ap_id, data_source_id)\")   \n",
    "copy_to_table(\"temp_obs.csv\", \"data_staging.observation_wifi(fingerprint_id)\")   \n",
    "copy_to_table(\"temp_detect.csv\", \"data_staging.ap_detection(ap_id, observation_wifi_fingerprint_id, rss)\")   \n",
    "\n",
    "    \n",
    "for insid in generated_ids:\n",
    "    file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"ap\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bluetooth devices and their observations\n",
    "\n",
    "if bluetoothobs_exists:\n",
    "\n",
    "    # Generating new ids \n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"truncate table data_staging.generated_ids;\")\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "    \n",
    "    with open('./temp.csv', 'w') as file:\n",
    "        for i in range(len([x for x in bluetooth_obs_frame.columns if 'BL-' in x])):\n",
    "            file.write(\"null\\n\")\n",
    "    copy_to_table(\"temp.csv\", \"data_staging.generated_ids(dummy_field)\")\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select id from data_staging.generated_ids\")\n",
    "    generated_ids = [x[0] for x in cursor.fetchall()]\n",
    "    connection.commit() \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    file_bt = open('./temp_bt.csv', 'w')\n",
    "    file_btds = open('./temp_btds.csv', 'w')\n",
    "    file_obs = open('./temp_obs.csv', 'w')\n",
    "    file_detect = open('./temp_detect.csv', 'w')    \n",
    "    \n",
    "    \n",
    "    # bt and bt_data_source\n",
    "    bt_map_id = {} # maps that keeps the correspondence between the bt name in the dataset and its newly generated ID in the database\n",
    "    for idx, btname in enumerate([x for x in bluetooth_obs_frame.columns if 'BL-' in x]):\n",
    "        macvalue = '' if str(btname.split('-')[2]) == 'NULL' else \"_\" + str(btname.split('-')[2]) \n",
    "        bt_map_id[btname] = generated_ids[idx]\n",
    "        file_bt.write(str(generated_ids[idx]) + \",\" + str(btname.split('-')[1]) + macvalue + \"\\n\")\n",
    "        file_btds.write(str(generated_ids[idx]) + \",\" + str(dsid) + \"\\n\")\n",
    "\n",
    "    \n",
    "    # Bluetooth observation and its detection\n",
    "\n",
    "    pbar = tqdm(total=len(bluetooth_obs_frame)*len([x for x in bluetooth_obs_frame.columns if 'BL-' in x]))\n",
    "    for _, fing in bluetooth_obs_frame.iterrows():\n",
    "        file_obs.write(str(fingerprint_code_map_id[str(fing.fingerprint_id)]) + \"\\n\")\n",
    "        for btname in [x for x in bluetooth_obs_frame.columns if 'BL-' in x]:\n",
    "            if not pd.isna(fing[btname]):\n",
    "                file_detect.write(str(bt_map_id[btname]) + \",\" + str(fingerprint_code_map_id[str(fing.fingerprint_id)]) + \",\" + str(fing[btname]) + \"\\n\")     \n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    file_bt.close()\n",
    "    file_btds.close()\n",
    "    file_obs.close()\n",
    "    file_detect.close()\n",
    "\n",
    "\n",
    "    copy_to_table(\"temp_bt.csv\", \"data_staging.bluetooth_device(id, name)\")   \n",
    "    copy_to_table(\"temp_btds.csv\", \"data_staging.bluetooth_device_data_source(bluetooth_device_id, data_source_id)\")   \n",
    "    copy_to_table(\"temp_obs.csv\", \"data_staging.observation_bluetooth(fingerprint_id)\")   \n",
    "    copy_to_table(\"temp_detect.csv\", \"data_staging.bluetooth_detection(bluetooth_device_id, observation_bluetooth_fingerprint_id, rss)\")   \n",
    "\n",
    "\n",
    "    for insid in generated_ids:\n",
    "        file_imported.write(str(insid) + \",\" + str(import_timestamp) + \",\" + str(dataset_name) + \",\" + \"bluetooth_device\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNSS and their observations\n",
    "\n",
    "if gnssobs_exists:\n",
    "    \n",
    "    file_gnss = open('./temp_gnss.csv', 'w')\n",
    "    \n",
    "    # Bluetooth observation and its detection\n",
    "\n",
    "    pbar = tqdm(total=len(gnss_obs_frame))\n",
    "    for i, row in gnss_obs_frame.iterrows():\n",
    "        file_gnss.write(str(fingerprint_code_map_id[str(row.fingerprint_id)]) + \",null,\" + format_nan_string_csv(row.latitude) + \",\" + format_nan_string_csv(row.longitude) + \",\" + format_nan_string_csv(row.elevation) + \",\" + format_nan_string_csv(row.num_satellites) + \"\\n\")     \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    \n",
    "    file_gnss.close()\n",
    "\n",
    "    copy_to_table(\"temp_gnss.csv\", \"data_staging.observation_gnss(fingerprint_id, is_valid, latitude, longitude, elevation, num_satellites)\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMU and their observations\n",
    "\n",
    "if imuobs_exists:\n",
    "    \n",
    "    file_imu = open('./temp_imu.csv', 'w')\n",
    "    file_imu_gyro = open('./temp_imu_gyro.csv', 'w')\n",
    "    file_imu_acce = open('./temp_imu_acce.csv', 'w')\n",
    "    file_imu_magn = open('./temp_imu_magn.csv', 'w')\n",
    "    \n",
    "    # IMU observation and its detection\n",
    "\n",
    "    unique_fings = imu_obs_frame['fingerprint_id'].drop_duplicates().values\n",
    "    pbar = tqdm(total=len(unique_fings))\n",
    "    for fing in unique_fings:\n",
    "        fing_frame = imu_obs_frame[imu_obs_frame['fingerprint_id'] == fing]\n",
    "        file_imu.write(str(fingerprint_code_map_id[str(fing)]) + \",null\\n\") \n",
    "        for i, row in fing_frame.iterrows():\n",
    "            ep_val = str(row.epoch) if not pd.isna(row.epoch) else str(i)\n",
    "            if not pd.isna(row.gyr_x):\n",
    "                file_imu_gyro.write(str(fingerprint_code_map_id[str(fing)]) + \",\" + ep_val + \",\" + str(row.gyr_x) + \",\" + str(row.gyr_y) + \",\" + str(row.gyr_z) + \"\\n\") \n",
    "            if not pd.isna(row.acc_x):\n",
    "                file_imu_acce.write(str(fingerprint_code_map_id[str(fing)]) + \",\" + ep_val + \",\" + str(row.acc_x) + \",\" + str(row.acc_y) + \",\" + str(row.acc_z) + \"\\n\") \n",
    "            if not pd.isna(row.mag_x):\n",
    "                file_imu_magn.write(str(fingerprint_code_map_id[str(fing)]) + \",\" + ep_val + \",\" + str(row.mag_x) + \",\" + str(row.mag_y) + \",\" + str(row.mag_z) + \"\\n\") \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    " \n",
    "    \n",
    "    file_imu.close()\n",
    "    file_imu_gyro.close()\n",
    "    file_imu_acce.close()\n",
    "    file_imu_magn.close()\n",
    "\n",
    "    copy_to_table(\"temp_imu.csv\", \"data_staging.observation_imu(fingerprint_id, is_valid)\")   \n",
    "    copy_to_table(\"temp_imu_gyro.csv\", \"data_staging.observation_imu_gyroscope(fingerprint_id, epoch, axis_x, axis_y, axis_z)\")   \n",
    "    copy_to_table(\"temp_imu_acce.csv\", \"data_staging.observation_imu_accelerometer(fingerprint_id, epoch, axis_x, axis_y, axis_z)\")   \n",
    "    copy_to_table(\"temp_imu_magn.csv\", \"data_staging.observation_imu_magnetometer(fingerprint_id, epoch, axis_x, axis_y, axis_z)\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify consistency of the new data\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select * from check_db_consistency('data_staging')\")\n",
    "record = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "display(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the content of the data staging area into the main database schema (public)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select data_staging.copy_tables()\")\n",
    "record = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()\n",
    "\n",
    "\n",
    "# Write down the imported IDs\n",
    "file_imported.close()\n",
    "copy_to_table(\"temp_imported.csv\", \"communication.imported_ids(id, import_time, source_name, entity_name)\")\n",
    "\n",
    "\n",
    "\n",
    "# Clean the data staging area at the end of the import process\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"select data_staging.truncate_tables('data_staging')\")\n",
    "record = cursor.fetchall()\n",
    "connection.commit() \n",
    "cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connections\n",
    "\n",
    "connection.close()\n",
    "\n",
    "\n",
    "ftp_client.close()\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
